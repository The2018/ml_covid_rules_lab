{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification rules for covid dataset\n",
    "\n",
    "\n",
    "## 1. Classification Rules Implementation\n",
    "\n",
    "### 1.1. PRISM algorithm\n",
    "In this lab we are going to implement the PRISM algorithm to extract the classification rules with the highest accuracy and coverage from the hospital patients dataset described in the class demo notebook.\n",
    "\n",
    "Our algorithm extracts the rules ranked by the accuracy (from highest to lowest), and the ties are broken by choosing the rule with higher coverage. If both accuracy and coverage are the same - the condition is selected arbitrarily.\n",
    "\n",
    "Your algorithm should have two additional optinal parameters:\n",
    "\n",
    "the accuracy threshold - the number from 0 to 1 which specifies when you need to stop refining the rule. I.e. if you reached this accuracy threshold, you do not need to add more conditions to the rule's antedescent.\n",
    "the coverage threshold - the absolute number of records covered by the rule. If the more precise rule covers less records than this threshold, the algorithm should stop refining this rule.\n",
    "The output of the algorithm should be a decision list, where all the rules are presented ranked as above, including the rule itself, its accuracy, and its coverage.\n",
    "\n",
    "Please keep your code clean, modular, and add explanations for each step.\n",
    "\n",
    "Finally, apply the algorithm to the COVID-19 dataset to learn reliable rules which determine which symptoms/preexisting conditions and their combination lead to the deadly outcomes of the COVID-19 infection.\n",
    "\n",
    "In your report (a separate markdown cell), summarize which rules did you learn from this dataset, and whether you were surpised by any of your discoveries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing set of observations `rows` based on the value `value` in column `column`.\n",
    "It can split on numeric value (greater than or equal) or on categorical value (equal/not equal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(rows, column, value):\n",
    "    # define split function according to the value type\n",
    "    split_function = None\n",
    "    if isinstance(value, int) or isinstance(value, float):\n",
    "        split_function = lambda row: row[column] >= value\n",
    "    else:\n",
    "        split_function = lambda row: row[column] == value\n",
    "\n",
    "    # Divide the rows into two sets and return them\n",
    "    set1 = [row for row in rows if split_function(row)]\n",
    "    set2 = [row for row in rows if not split_function(row)]\n",
    "    return (set1, set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the quality of the split, we need to count the occurrence of each class label in the current subset (potential tree node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels(rows):\n",
    "    label_count = {}\n",
    "    for row in rows:\n",
    "        # The class label is in the last column\n",
    "        label = row[- 1]\n",
    "        if label not in label_count:\n",
    "            label_count[label] = 0\n",
    "        label_count[label] += 1\n",
    "    return label_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different methods to evaluate the homogeneity of the target variable within a given subset generated by split.\n",
    "\n",
    "* GINI:  probability that two items randomly selected fall into the same class. We use here GINI impurity, because we want to use the same code which chooses the best split based on the **minimum** of the score. GINI impurity = 1.0 - GINI.\n",
    "* Entropy: the sum of $-p(x)\\log(p(x))$ across all the different possible classes $x$.\n",
    "* Variance: variance is used if the target variable can be considered a number. In this case we build a special type of decision tree - a **Regression Tree**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_one_rule(data, accu_threshold, coverage_threshold):\n",
    "    # Find class labels and attributes from data\n",
    "    columns = list(data.columns)\n",
    "    attributes = columns[:-1]\n",
    "    class_labels = data[columns[-1]].unique()\n",
    "    \n",
    "    # Find the rule for the dataset for each current class label\n",
    "    for clas in class_labels:\n",
    "        cur_data = data[columns[-1]=clas]\n",
    "        rule_lhs = np.array()\n",
    "        rule = np.array(rule_lhs,clas)\n",
    "        \n",
    "        while accuracy(rule) > accu_threshold or cur_attr.length == 0:\n",
    "            \n",
    "            # For each attribute a not mentioned in the rule, and each attr value v, consider adding the condition a = v to rule_lhs\n",
    "            cur_attr = attributes\n",
    "            best_accur = 0\n",
    "            best_pair = []\n",
    "            for a in cur_attr:\n",
    "                values = data[a].unique()\n",
    "                for v in values:\n",
    "                    rule[0].append([a,v])\n",
    "                    \n",
    "                    # select the rule of the attribute,value pair with best accuracy\n",
    "                    if accuracy(rule) > best_accur:\n",
    "                        best_accur = accuracy(rule)\n",
    "                        best_pair = [a,v]\n",
    "                    elif accuracy(rule) == best_accur:\n",
    "                        cur_correct = data[]\n",
    "                        correst_so_far = data[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need to build the decision tree from data.\n",
    "\n",
    "The `buildtree` function below takes as an input all the rows of the dataset and returns a decision tree with the root  implemented as `DecisionNode` class.\n",
    "\n",
    "The default `score_func` to test the quality of the split is entropy, but any other score can be applied by passing any of the scoring functions above. Remember that `variance` can only be applied if the target variable is numeric (Regression Tree).\n",
    "\n",
    "To avoid overfitting, the algorithm uses additional stop criteria: if the improvement of the score is less than `min_improvement` or if the number of samples in every node is less than `min_samples`, then the splitting of samples stops. For the sake of the visual demo, there is also a parameter called `max_depth`, which controls the maximum depth of the tree. \n",
    "\n",
    "With the default values the algorithm will continue until the score of the split stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildtree(rows, score_func=entropy, min_improvement=0, min_samples=0, max_depth=None, depth=0):\n",
    "    if len(rows) == 0:\n",
    "        return DecisionNode()\n",
    "    # Compute overall score for the entire rows dataset\n",
    "    current_score = score_func(rows)\n",
    "\n",
    "    # Set up accumulator variables to track the best split criteria\n",
    "    best_score = current_score\n",
    "    best_criteria = None\n",
    "    best_sets = None\n",
    "    \n",
    "    # Total number of features - except the last column where we store the class (target)\n",
    "    column_count = len(rows[0]) - 1 \n",
    "    for col in range(0, column_count):\n",
    "        # Generate the list of unique values in\n",
    "        # this column to split on them\n",
    "        column_values = set()\n",
    "        for row in rows:\n",
    "            column_values.add(row[col])\n",
    "            \n",
    "        # Now try splitting the rows \n",
    "        # on each unique value in this column\n",
    "        for value in column_values:\n",
    "            (set1, set2) = split(rows, col, value)\n",
    "\n",
    "            # Evaluate the quality of the split\n",
    "            # p is the proportion of subset set1 \n",
    "            p = float(len(set1)) / len(rows)\n",
    "            split_score = p * score_func(set1) + (1-p) * score_func(set2)\n",
    "            \n",
    "            if split_score < best_score and \\\n",
    "                (len(set1) > min_samples and len(set2) > min_samples) and \\\n",
    "                (current_score - split_score) > min_improvement:\n",
    "                best_score = split_score\n",
    "                best_criteria = (col, value)\n",
    "                best_sets = (set1, set2)\n",
    "\n",
    "    # Create the sub branches\n",
    "    if (current_score - best_score) > min_improvement and \\\n",
    "        (max_depth is None or depth < max_depth) :\n",
    "        # print(\"Splitting on\",best_criteria, \" 2 sets:\", len(best_sets[0]),len(best_sets[1]))\n",
    "        true_branch = buildtree(best_sets[0], score_func, min_improvement, min_samples, max_depth, depth+1)\n",
    "        false_branch = buildtree(best_sets[1], score_func, min_improvement, min_samples, max_depth, depth+1)\n",
    "        return DecisionNode(col=best_criteria[0], value=best_criteria[1],\n",
    "                            tb=true_branch, fb=false_branch)\n",
    "    else: # Done splitting - summarize class labels in leaf nodes\n",
    "        return DecisionNode(results=count_labels(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using learned tree for classification\n",
    "\n",
    "### 2.1. Classifying fruits\n",
    "\n",
    "First, build the tree from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fruits with their size and color\n",
    "fruits = [\n",
    "    [4, 'red', 'apple'],\n",
    "    [4, 'green', 'apple'],\n",
    "    [1.5, 'red', 'cherry'],\n",
    "    [1, 'green', 'grape'],\n",
    "    [5, 'red', 'apple'],\n",
    "    [1.2, 'red', 'grape']\n",
    "]\n",
    "\n",
    "tree = buildtree(fruits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `prediction` function below outputs the label of the leaf node according to a prevalent class.\n",
    "It cannot be used for regression trees - you need to implement your own function if you want to use this code for predicting a numeric target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(leaf_labels):\n",
    "    total = 0\n",
    "    result = {}\n",
    "    for label, count in leaf_labels.items():\n",
    "        total += count\n",
    "        result[label] = count\n",
    "\n",
    "    for label, val in result.items():\n",
    "        result[label] = str(int(result[label]/total * 100))+\"%\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Coronavirus risk factors\n",
    "\n",
    "As discussed in the lecture, decision trees can be used not only for classification, but also to find out which atttributes are most important in classifying the record into a specific class. In this part we want to find out which symptoms/chronic conditions contribute most to the deadly outcome from catching COVID-19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tis Mexican dataset which contains the information from the Statistical Yearbooks of Morbidity 2015-2017 (as well as the information regarding cases associated with COVID-19) was found on [kaggle](https://www.kaggle.com/tanmoyx/covid19-patient-precondition-dataset).\n",
    "\n",
    "Download the preprocessed dataset which contains only patients that tested positive for COVID-19 and with symptom atributes converted to categorical: [link](https://docs.google.com/spreadsheets/d/1EPewR1KdT8mszXJMuqELRTHAVIFHEJw9VwLsb9whKPI/edit?usp=sharing).\n",
    "\n",
    "In this dataset we have the following attributes:\n",
    "1. sex: 1 -woman, 2-man\n",
    "2. age: numeric\n",
    "3. diabetes: yes/no\n",
    "4. copd (chronic obstructive pulmonary disease): yes/no\n",
    "5. asthma: yes/no\n",
    "6. imm_supr (suppressed immune system): yes/no\n",
    "7. hypertension: yes/no\n",
    "8. cardiovascular: yes/no\n",
    "9. renal_chronic: yes/no\n",
    "10. tobacco: yes/no\t\n",
    "11. outcome: alive/dead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../../data_ml_2020/covid_categorical_good.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(data_file)\n",
    "data = data.dropna(how=\"any\")\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rows = data.to_numpy().tolist()\n",
    "len(data_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = data.columns.to_numpy().tolist()\n",
    "print(columns_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Using custom decision tree algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = buildtree(data_rows, score_func=entropy, min_improvement=0, min_samples=0, max_depth=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(tree, '', columns_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Using sklearn\n",
    "\n",
    "The decision tree algorithm in sklearn library is not implemented very well. It requires all the attributes to be numeric - while decision trees work best with the categorical attributes. The dataset for this part contains all numeric attributes and can be found [here](https://docs.google.com/spreadsheets/d/1FHTP2RtclUg05GztDW-diMbajMAynPnECW8hyjNLUys/edit?usp=sharing).\n",
    "\n",
    "In this dataset the binary yes/no attributes are presented as:\n",
    "* YES: 1\n",
    "* NO: 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file =  \"../../data_ml_2020/covid_numeric.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['sex', 'diabetes', 'copd', 'asthma', 'imm_supr', 'hypertension',\n",
    "       'cardiovascular', 'obesity', 'renal_chronic', 'tobacco']:\n",
    "    data_num[k].replace({97: np.nan, 98: np.nan, 99: np.nan}, inplace=True)\n",
    "\n",
    "\n",
    "data_num = data_num.dropna()\n",
    "len(data_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see if there is a correlation between any of the attributes and the outcome. We will replace the \"alive\" outcome with number 0, and \"dead\" with number 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data_num_ = data_num.copy()\n",
    "conditions = [ data_num_['outcome'].eq(\"alive\"), data_num_['outcome'].eq('dead')]\n",
    "choices = [0,2]\n",
    "\n",
    "data_num_['outcome'] = np.select(conditions, choices, default=np.nan)\n",
    "\n",
    "corr = data_num_.corr()\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide dataset into features and class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_num.loc[:, data_num.columns != 'outcome']\n",
    "print(X.columns)\n",
    "\n",
    "Y = data_num.loc[:, data_num.columns == 'outcome']\n",
    "print(Y.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset intro training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different parameters can be specified to build the model:\n",
    "\n",
    "`model = tree.DecisionTreeClassifier(\n",
    "        criterion='entropy', \n",
    "        max_depth=None, \n",
    "        min_samples_split=2, \n",
    "        min_samples_leaf=1, \n",
    "        max_features=None, \n",
    "        random_state=None, \n",
    "        min_density=None, \n",
    "        compute_importances=None, \n",
    "        max_leaf_nodes=None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(criterion='entropy', max_depth = 7)\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = data.columns.to_numpy().tolist()\n",
    "from sklearn.tree import export_text\n",
    "r = export_text(model, feature_names=columns_list[:-1])\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most important comorbidity factors? Hard to tell. \n",
    "We will try to discover them more efficiently in the next lab using classification rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2020 Marina Barsky. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
